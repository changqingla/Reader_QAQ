"""Node implementations for the agent graph."""
from typing import Dict, Any

from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI

from .state import AgentState, IntentType, StepType, ExecutionResult, QAPair
from ..prompts import (
    INTENT_RECOGNITION_PROMPT,
    get_planning_prompt,
    TOOL_EXECUTION_PROMPT,
    INFORMATION_ANALYSIS_PROMPT,
    SIMPLE_INTERACTION_PROMPT,
    get_answer_prompt,
    SUB_QUESTION_ANSWER_PROMPT,
    get_sub_question_context
)
from ..utils.logger import get_logger
from ..utils.json_parser import parse_json_response
from ..tools import RecallTool, WebSearchTool
from config import get_settings

# ä¸Šä¸‹æ–‡ç®¡ç†æ¨¡å— - å¼ºåˆ¶ä¾èµ–
from context.session_manager import SessionManager
from context.session_storage import SessionStorage
from context.context_injector import ContextInjector

logger = get_logger(__name__)
settings = get_settings()


class AgentNodes:
    """Container for all agent node functions."""
    
    def __init__(
        self,
        llm: ChatOpenAI,
        recall_tool: RecallTool,
        web_search_tool: WebSearchTool = None
    ):
        """
        Initialize agent nodes.
        
        Args:
            llm: Language model for generation
            recall_tool: Tool for document retrieval
            web_search_tool: Optional tool for web search
        """
        self.llm = llm
        self.recall_tool = recall_tool
        self.web_search_tool = web_search_tool
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆå¼ºåˆ¶ä¾èµ–ï¼‰
        storage = SessionStorage()
        self.session_manager = SessionManager(storage)
        self.context_injector = ContextInjector()
        
        logger.info("AgentNodes initialized with session management")
    
    def _execute_recall(
        self,
        query: str,
        state: AgentState
    ) -> str:
        """
        Execute recall with dynamic parameters from state.
        
        Args:
            query: Search query
            state: Agent state containing optional recall parameters
            
        Returns:
            Recall results
        """
        # Get dynamic parameters from state if provided
        index_names = state.get('recall_index_names')
        doc_ids = state.get('recall_doc_ids')
        
        # Call recall tool's _run method directly with optional parameters
        return self.recall_tool._run(
            query=query,
            index_names=index_names,
            doc_ids=doc_ids
        )
    
    def _get_conversation_context(
        self,
        state: AgentState,
        num_turns: int = 2,
        stage: str = "intent_recognition"
    ) -> str:
        """
        è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        
        Args:
            state: Agent state
            num_turns: éœ€è¦çš„å¯¹è¯è½®æ¬¡æ•°ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹ï¼‰
            stage: å¤„ç†é˜¶æ®µï¼ˆintent_recognition, planning, answer_generationï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯å†å²å­—ç¬¦ä¸²
        """
        session_id = state.get('session_id')
        if not session_id:
            logger.warning("No session_id provided, cannot retrieve context")
            return ""
        
        # æ ¹æ®é˜¶æ®µé€‰æ‹©åˆé€‚çš„æ³¨å…¥æ–¹æ³•
        if stage == "intent_recognition":
            messages = self.context_injector.inject_for_intent_recognition(session_id)
        elif stage == "planning":
            messages = self.context_injector.inject_for_planning(session_id)
        elif stage == "answer_generation":
            messages = self.context_injector.inject_for_answer_generation(session_id)
        elif stage == "simple_interaction":
            messages = self.context_injector.inject_for_simple_interaction(session_id)
        else:
            logger.warning(f"Unknown stage: {stage}")
            messages = []
        
        if not messages:
            return ""
        
        # ä½¿ç”¨ContextInjectorçš„æ ¼å¼åŒ–æ–¹æ³•
        return self.context_injector.format_messages_for_prompt(messages)
    
    def _format_execution_history(self, execution_results: list) -> str:
        """
        Format execution history for replanning context.
        
        Args:
            execution_results: List of execution results
            
        Returns:
            Formatted execution history string
        """
        if not execution_results:
            return "æ— æ‰§è¡Œå†å²"
        
        history = []
        for r in execution_results:
            status = "âœ… æˆåŠŸ" if not r.get('error') else f"âŒ å¤±è´¥: {r.get('error')}"
            tool = r.get('tool_used') or 'æ— å·¥å…·'
            query = r.get('query') or 'æ— æŸ¥è¯¢'
            result = r.get('result') or ''
            result_preview = result[:200] if result else 'æ— ç»“æœ'
            
            # å®‰å…¨å¤„ç†queryåˆ‡ç‰‡
            query_str = str(query) if query else 'æ— æŸ¥è¯¢'
            query_preview = query_str[:150] + ('...' if len(query_str) > 150 else '')
            
            history.append(
                f"ã€æ­¥éª¤ {r['step_index']+1}ã€‘{r['step_title']} (ç±»å‹: {r['step_type']})\n"
                f"  - ä½¿ç”¨å·¥å…·: {tool}\n"
                f"  - æŸ¥è¯¢å†…å®¹: {query_preview}\n"
                f"  - æ‰§è¡ŒçŠ¶æ€: {status}\n"
                f"  - ç»“æœé¢„è§ˆ: {result_preview}{'...' if len(result_preview) >= 200 else ''}"
            )
        return "\n\n".join(history)
    
    def _build_replanning_context(self, state: AgentState, replan_count: int) -> str:
        """
        Build replanning context with execution history and analysis results.
        
        Args:
            state: Current agent state
            replan_count: Number of replanning attempts
            
        Returns:
            Formatted replanning context string
        """
        plan = state.get('plan', {})
        analysis = state.get('analysis_result', {})
        execution_results = state.get('execution_results', [])
        collected_info = state.get('collected_information', '')
        
        context_parts = [
            "=" * 80,
            f"ğŸ”„ é‡è¦æç¤ºï¼šè¿™æ˜¯ç¬¬ {replan_count} æ¬¡é‡æ–°è§„åˆ’",
            "=" * 80,
            "",
            "ã€ä¸Šä¸€æ¬¡è§„åˆ’çš„æƒ…å†µã€‘",
            f"è§„åˆ’æ ‡é¢˜: {plan.get('title', 'æ— ')}",
            f"è§„åˆ’æ€è·¯: {plan.get('thought', 'æ— ')[:200]}...",
            f"æ‰§è¡Œæ­¥éª¤æ•°: {len(plan.get('steps', []))} æ­¥",
            "",
            "ã€æ‰§è¡Œå†å²è¯¦æƒ…ã€‘",
            self._format_execution_history(execution_results),
            "",
            "ã€ä¿¡æ¯å……åˆ†æ€§åˆ†æã€‘",
            f"åˆ†æç»“è®º: {analysis.get('analysis', 'æ— åˆ†æç»“æœ')}",
            ""
        ]
        
        # æ·»åŠ ç¼ºå¤±æ–¹é¢
        missing_aspects = analysis.get('missing_aspects', [])
        if missing_aspects:
            context_parts.append("ã€â— ç¼ºå¤±çš„å…³é”®ä¿¡æ¯ã€‘")
            for i, aspect in enumerate(missing_aspects, 1):
                context_parts.append(f"  {i}. {aspect}")
            context_parts.append("")
        
        # æ·»åŠ å»ºè®®è¡ŒåŠ¨
        suggested_actions = analysis.get('suggested_actions', [])
        if suggested_actions:
            context_parts.append("ã€ğŸ’¡ å»ºè®®çš„è¡¥å……è¡ŒåŠ¨ã€‘")
            for i, action in enumerate(suggested_actions, 1):
                context_parts.append(f"  {i}. {action}")
            context_parts.append("")
        
        # æ·»åŠ å·²æ”¶é›†ä¿¡æ¯çš„æ‘˜è¦
        if collected_info and collected_info.strip():
            info_preview = collected_info[:500] + "..." if len(collected_info) > 500 else collected_info
            context_parts.extend([
                "ã€å·²æ”¶é›†çš„ä¿¡æ¯æ‘˜è¦ã€‘",
                info_preview,
                ""
            ])
        
        context_parts.extend([
            "=" * 80,
            "ğŸ“ é‡æ–°è§„åˆ’æŒ‡å¯¼ï¼š",
            "1. åˆ†æä¸Šè¿°æ‰§è¡Œå†å²ï¼Œæ‰¾å‡ºä¸ºä»€ä¹ˆä¿¡æ¯ä¸å……åˆ†",
            "2. é‡ç‚¹å…³æ³¨ã€Œç¼ºå¤±çš„å…³é”®ä¿¡æ¯ã€å’Œã€Œå»ºè®®çš„è¡¥å……è¡ŒåŠ¨ã€",
            "3. è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼š",
            "   - å¦‚æœä¹‹å‰çš„æŸ¥è¯¢è¯å¤ªå®½æ³›ï¼Œä½¿ç”¨æ›´å…·ä½“çš„æŸ¥è¯¢",
            "   - å¦‚æœä¹‹å‰çš„æŸ¥è¯¢è¯å¤ªå…·ä½“ï¼Œå°è¯•æ›´å®½æ³›çš„æŸ¥è¯¢",
            "   - å¦‚æœæŸä¸ªæ–¹é¢å®Œå…¨æ²¡æœ‰æ£€ç´¢ï¼Œå¢åŠ å¯¹åº”çš„recallæ­¥éª¤",
            "   - è€ƒè™‘ä½¿ç”¨ä¸åŒçš„æ£€ç´¢è§’åº¦æˆ–å…³é”®è¯",
            "4. é¿å…é‡å¤ä¹‹å‰å¤±è´¥çš„ç­–ç•¥",
            "5. å¦‚æœæ˜¯æ–‡æ¡£å†…å®¹é—®é¢˜ï¼Œè€ƒè™‘ç”¨æ›´æ˜ç¡®çš„é—®æ³•æ£€ç´¢",
            "=" * 80,
            ""
        ])
        
        return "\n".join(context_parts)
    
    def intent_recognition_node(self, state: AgentState) -> Dict[str, Any]:
        """
        Recognize user intent from query.
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with detected intent
        """
        logger.info("============ Intent Recognition Node ============")
        logger.info(f"User query: {state['user_query'][:100]}...")
        logger.info(f"Previous messages: {len(state.get('messages', []))}")
        
        try:
            # Check if mode_type is provided
            if state.get("mode_type"):
                mode_type = state["mode_type"]
                logger.info(f"Using provided mode_type: {mode_type}")
                
                # Validate it's a valid IntentType
                try:
                    detected_intent = IntentType(mode_type)
                    logger.info(f"Validated intent: {detected_intent}")
                    return {
                        "detected_intent": detected_intent,
                        "messages": state.get("messages", []) + [
                            HumanMessage(content=state["user_query"])
                        ]
                    }
                except ValueError:
                    logger.warning(f"Invalid mode_type: {mode_type}, falling back to LLM")
            
            # Build context-aware prompt for intent recognition
            query_with_context = state["user_query"]
            
            # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆ2è½®ï¼‰
            context_str = self._get_conversation_context(state, num_turns=2, stage="intent_recognition")
            
            if context_str:
                query_with_context = f"å¯¹è¯å†å²ï¼š\n{context_str}\n\nå½“å‰é—®é¢˜ï¼š{state['user_query']}"
                logger.info("Using conversation history for intent recognition (2 turns)")
            
            # Use LLM to detect intent
            prompt = INTENT_RECOGNITION_PROMPT.format(user_query=query_with_context)
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # Parse JSON response
            intent_data = parse_json_response(response.content, expected_fields=["intent"])
            
            if intent_data:
                intent_str = intent_data.get("intent")
                confidence = intent_data.get("confidence", "unknown")
                reasoning = intent_data.get("reasoning", "")
                
                # æ˜¾ç¤ºå¤§æ¨¡å‹åˆ¤æ–­çš„ä»»åŠ¡ç±»å‹
                logger.info("="*60)
                logger.info("ğŸ¯ å¤§æ¨¡å‹åˆ¤æ–­ç»“æœ:")
                logger.info(f"   ä»»åŠ¡ç±»å‹: {intent_str}")
                logger.info(f"   ç½®ä¿¡åº¦: {confidence}")
                logger.info(f"   åˆ¤æ–­ä¾æ®: {reasoning}")
                logger.info("="*60)
            else:
                # Fallback: try to extract intent from plain text
                intent_str = response.content.strip()
                logger.warning(f"Failed to parse JSON, trying plain text: {intent_str}")
            
            try:
                detected_intent = IntentType(intent_str)
            except ValueError:
                logger.warning(f"Invalid intent from LLM: {intent_str}, defaulting to knowledge_reasoning")
                detected_intent = IntentType.KNOWLEDGE_REASONING
            
            return {
                "detected_intent": detected_intent,
                "messages": state.get("messages", []) + [
                    HumanMessage(content=state["user_query"])
                ]
            }
            
        except Exception as e:
            logger.error(f"Error in intent recognition: {str(e)}", exc_info=True)
            raise RuntimeError(f"Intent recognition failed: {str(e)}")
    
    def simple_interaction_node(self, state: AgentState) -> Dict[str, Any]:
        """
        Handle simple interactions directly.
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with final answer
        """
        logger.info("============ Simple Interaction Node ============")
        
        try:
            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šæ³¨å…¥å¯¹è¯å†å²ï¼Œè®©Agentèƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯
            query_with_context = state["user_query"]
            
            # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘2è½®å¯¹è¯ï¼‰
            context_str = self._get_conversation_context(state, num_turns=0, stage="simple_interaction")
            
            if context_str:
                query_with_context = f"ã€å¯¹è¯å†å²ã€‘\n{context_str}\n\nã€å½“å‰é—®é¢˜ã€‘\n{state['user_query']}"
                logger.info("âœ… Simple interaction using ALL active conversation history (with token limit protection)")
            else:
                logger.info("âš ï¸  No conversation history available")
            
            prompt = SIMPLE_INTERACTION_PROMPT.format(user_query=query_with_context)
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            final_answer = response.content
            logger.info(f"Generated simple response: {final_answer[:100]}...")
            
            # ä¼šè¯ç®¡ç†ï¼šä¿å­˜æ¶ˆæ¯ï¼ˆsimple_interactionè·³è¿‡äº†planning/executionï¼‰
            session_id = state.get('session_id')
            if session_id:
                # ğŸ”‘ å…³é”®ï¼šä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆåœ¨workflowç»“æŸæ—¶ä¿å­˜ï¼Œè€Œä¸æ˜¯å¼€å§‹æ—¶ï¼‰
                if not state.get('_user_message_saved'):
                    self.session_manager.add_user_message(
                        session_id=session_id,
                        content=state["user_query"]
                    )
                    logger.info(f"User message saved to session {session_id}")
                
                # ä¿å­˜åŠ©æ‰‹å›å¤
                self.session_manager.add_assistant_message(
                    session_id=session_id,
                    content=final_answer
                )
                logger.info(f"âœ… Assistant message saved to session {session_id}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
                if self.session_manager.should_compress(session_id):
                    logger.info(f"Triggering compression for session {session_id}")
                    compression_record = self.session_manager.trigger_compression(session_id)
                    logger.info(
                        f"Compression completed: saved {compression_record.saved_tokens} tokens, "
                        f"round {compression_record.round}"
                    )
            
            return {
                "final_answer": final_answer,
                "messages": state.get("messages", []) + [
                    AIMessage(content=final_answer)
                ]
            }
            
        except Exception as e:
            logger.error(f"Error in simple interaction: {str(e)}", exc_info=True)
            raise RuntimeError(f"Simple interaction failed: {str(e)}")
    
    def plan_generation_node(self, state: AgentState) -> Dict[str, Any]:
        """
        Generate execution plan based on intent.
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with execution plan
        """
        replan_count = state.get("replan_count", 0)
        
        logger.info("================ Plan Generation Node ================")
        logger.info(f"Intent: {state['detected_intent']}")
        logger.info(f"Replan count: {replan_count}")
        
        deep_thinking = state.get("deep_thinking", False)
        logger.info(f"Planning mode: {'Deep Thinking' if deep_thinking else 'Fast'}")
        
        try:
            # Build context-aware query
            query_with_context = state["user_query"]
            
            # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆ2è½®ï¼‰
            context_str = self._get_conversation_context(state, num_turns=2, stage="planning")
            
            if context_str:
                query_with_context = f"å¯¹è¯å†å²ï¼ˆç”¨äºç†è§£ä»£è¯å’Œä¸Šä¸‹æ–‡ï¼‰ï¼š\n{context_str}\n\nå½“å‰é—®é¢˜ï¼š{state['user_query']}"
                logger.info("Using conversation history for planning (2 turns)")
            
            # ğŸ”‘ å…³é”®ä¼˜åŒ–ï¼šå¦‚æœæ˜¯é‡æ–°è§„åˆ’ï¼Œæ·»åŠ æ‰§è¡Œå†å²å’Œåˆ†æç»“æœ
            if replan_count > 0:
                replanning_context = self._build_replanning_context(state, replan_count)
                query_with_context = f"{replanning_context}\n\n{query_with_context}"
                logger.info(f"Added replanning context (attempt {replan_count})")
                logger.info("=" * 60)
                logger.info("ğŸ”„ Replanning Context Preview:")
                logger.info(replanning_context[:500] + "...")
                logger.info("=" * 60)
            
            # Get the appropriate planning prompt based on intent and mode
            prompt_template = get_planning_prompt(
                intent_type=state["detected_intent"],
                deep_thinking=deep_thinking
            )
            
            # Format prompt with user query
            prompt = prompt_template.format(user_query=query_with_context)
            
            # Generate plan
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # Parse JSON response
            plan = parse_json_response(
                response.content,
                expected_fields=["locale", "thought", "title", "steps"]
            )
            
            if not plan:
                raise ValueError("Failed to parse plan JSON from LLM response")
            
            # logger.info(f"Generated plan: {plan}")
            # logger.info(f"Generated plan: {plan['title']}")
            # logger.info(f"Number of steps: {len(plan['steps'])}")
            
            # Print complete plan details
            logger.info("=" * 60)
            logger.info("ğŸ“‹ å®Œæ•´ä»»åŠ¡è§„åˆ’")
            logger.info("=" * 60)
            logger.info(f"æ ‡é¢˜: {plan['title']}")
            logger.info(f"\næ€è€ƒè¿‡ç¨‹:\n{plan['thought']}")
            logger.info(f"\næ‰§è¡Œæ­¥éª¤ (å…±{len(plan['steps'])}æ­¥):")
            for i, step in enumerate(plan['steps'], 1):
                logger.info(f"æ­¥éª¤ {i}. [{step['step_type']}] {step['title']}")
            logger.info("=" * 60)
            
            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šé‡æ–°è§„åˆ’æ—¶ä¿ç•™ä¹‹å‰æ”¶é›†çš„ä¿¡æ¯
            # åªé‡ç½®å½“å‰è§„åˆ’çš„æ‰§è¡ŒçŠ¶æ€ï¼Œä¸æ¸…ç©ºå·²æ”¶é›†çš„ä¿¡æ¯
            if replan_count > 0:
                # é‡æ–°è§„åˆ’ï¼šä¿ç•™å·²æ”¶é›†çš„ä¿¡æ¯å’Œæ‰§è¡Œå†å²
                logger.info("ğŸ”„ ä¿å­˜ä¹‹å‰çš„æ‰§è¡Œç»“æœå’Œæ”¶é›†çš„ä¿¡æ¯")
                return {
                    "plan": plan,
                    "current_step_index": 0,
                    # ä¿ç•™ä¹‹å‰çš„æ‰§è¡Œç»“æœï¼ˆç”¨äºå†å²è®°å½•ï¼‰
                    "execution_results": state.get("execution_results", []),
                    # ä¿ç•™ä¹‹å‰æ”¶é›†çš„ä¿¡æ¯ï¼ˆç´¯ç§¯ï¼‰
                    "collected_information": state.get("collected_information", "")
                }
            else:
                # é¦–æ¬¡è§„åˆ’ï¼šåˆå§‹åŒ–ä¸ºç©º
                return {
                    "plan": plan,
                    "current_step_index": 0,
                    "execution_results": [],
                    "collected_information": ""
                }
            
        except Exception as e:
            logger.error(f"Error in plan generation: {str(e)}", exc_info=True)
            raise RuntimeError(f"Plan generation failed: {str(e)}")
    
    def execution_node(self, state: AgentState) -> Dict[str, Any]:
        """
        Execute current step in the plan.
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with execution results
        """
        plan = state["plan"]
        current_step_index = state["current_step_index"]
        current_step = plan["steps"][current_step_index]
        
        logger.info(f"============ Execution Node - Step {current_step_index + 1}/{len(plan['steps'])} ============")
        logger.info(f"Step title: {current_step['title']}")
        logger.info(f"Step type: {current_step['step_type']}")
        
        try:
            # Build context-aware user query
            # æ³¨æ„ï¼šexecutioné˜¶æ®µé…ç½®ä¸ºä¸æ³¨å…¥å†å²ï¼ˆexecution_turns=0ï¼‰
            # å› ä¸ºå·¥å…·æ‰§è¡Œä¸»è¦åŸºäºå½“å‰æ­¥éª¤çš„æ˜ç¡®æŒ‡ä»¤ï¼Œä¸éœ€è¦å®Œæ•´å¯¹è¯å†å²
            user_query_with_context = state["user_query"]
            
            # Prepare tool execution prompt
            use_direct_content = state.get("use_direct_content", False)
            
            # ğŸ”‘ ä¼˜åŒ–ï¼šåœ¨ç›´æ¥å†…å®¹æ¨¡å¼ä¸‹ï¼Œæ˜ç¡®å‘ŠçŸ¥ LLM
            collected_info = state.get("collected_information", "æš‚æ— ")
            if use_direct_content and collected_info != "æš‚æ— ":
                collected_info = f"ğŸ“„ **å·²æä¾›å®Œæ•´æ–‡æ¡£å†…å®¹**ï¼ˆç›´æ¥å†…å®¹æ¨¡å¼ï¼Œæ— éœ€å†æ¬¡ recallï¼‰\n\n{collected_info}"
            
            prompt = TOOL_EXECUTION_PROMPT.format(
                user_query=user_query_with_context,
                step_title=current_step["title"],
                step_type=current_step["step_type"],
                step_index=current_step_index + 1,
                total_steps=len(plan["steps"]),
                collected_information=collected_info,
                web_search_available="å¯ç”¨" if state.get("enable_web_search") and self.web_search_tool else "ä¸å¯ç”¨"
            )
            
            # Get tool decision
            response = self.llm.invoke([HumanMessage(content=prompt)])
            decision = parse_json_response(
                response.content,
                expected_fields=["need_tool"]  # Only require need_tool field
            )
            
            if not decision:
                logger.error(f"Failed to parse tool decision. Response: {response.content[:500]}")
                # Fallback: default behavior based on step type
                if current_step["step_type"] == "recall":
                    # For recall steps, default to calling recall tool
                    decision = {
                        "need_tool": True,
                        "tool_name": "recall",
                        "query": f"{state['user_query']} - {current_step['title']}",
                        "reasoning": "Fallback: Auto-generated query for recall step"
                    }
                    logger.warning("Using fallback decision for recall step")
                else:
                    # For other steps, skip tool if we can't parse decision
                    decision = {
                        "need_tool": False,
                        "tool_name": None,
                        "query": None,
                        "reasoning": "Fallback: Skipping tool due to parse error"
                    }
                    logger.warning("Using fallback decision: skipping tool")
            
            logger.info(f"Tool decision: {decision['reasoning']}")
            
            # ğŸ”‘ Check if direct content mode is enabled for recall steps
            use_direct_content = state.get("use_direct_content", False)
            direct_content = state.get("direct_content")
            
            if use_direct_content and current_step["step_type"] == "recall":
                logger.info("=" * 60)
                logger.info("ğŸ“„ ç›´æ¥å†…å®¹æ¨¡å¼ï¼šè·³è¿‡ recall å·¥å…·ï¼Œä½¿ç”¨æä¾›çš„æ–‡æ¡£å†…å®¹")
                logger.info(f"   å†…å®¹é•¿åº¦: {len(direct_content):,} å­—ç¬¦")
                logger.info(f"   Token æ•°: {state.get('content_token_count', 'N/A')}")
                logger.info("=" * 60)
                
                # Create execution result with direct content
                execution_result: ExecutionResult = {
                    "step_index": current_step_index,
                    "step_title": current_step["title"],
                    "step_type": StepType.RECALL,
                    "tool_used": "direct_content",
                    "query": "ä½¿ç”¨ç”¨æˆ·æä¾›çš„å®Œæ•´æ–‡æ¡£å†…å®¹",
                    "result": direct_content,
                    "error": None
                }
                
                # Skip normal tool execution, go directly to result handling
                # (continue with the existing code below)
            else:
                # Prepare execution result for normal tool execution
                execution_result: ExecutionResult = {
                    "step_index": current_step_index,
                    "step_title": current_step["title"],
                    "step_type": StepType(current_step["step_type"]),
                    "tool_used": decision.get("tool_name"),
                    "query": decision.get("query"),
                    "result": "",
                    "error": None
                }
            
            # Execute tool if needed (only when not using direct content)
            if not (use_direct_content and current_step["step_type"] == "recall") and decision["need_tool"] and decision.get("tool_name"):
                tool_name = decision["tool_name"]
                query = decision["query"]
                
                logger.info(f"Calling tool: {tool_name} with query: {query[:200]}...")
                
                try:
                    if tool_name == "recall":
                        tool_result = self._execute_recall(query, state)
                    elif tool_name == "web_search":
                        if not self.web_search_tool:
                            raise RuntimeError("Web search tool is not available")
                        tool_result = self.web_search_tool.run(query)
                    else:
                        raise ValueError(f"Unknown tool: {tool_name}")
                    
                    execution_result["result"] = tool_result
                    logger.info(f"Tool execution successful, result length: {len(tool_result)}")
                    
                except Exception as tool_error:
                    error_msg = f"Tool execution error: {str(tool_error)}"
                    logger.error(error_msg, exc_info=True)
                    execution_result["error"] = error_msg
                    execution_result["result"] = f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(tool_error)}"
                    
            else:
                # ğŸ”‘ ä¿®å¤ï¼šå¦‚æœæ˜¯ç›´æ¥å†…å®¹æ¨¡å¼ï¼Œresult å·²ç»è¢«è®¾ç½®ä¸ºå®Œæ•´æ–‡æ¡£ï¼Œä¸è¦è¦†ç›–
                if not (use_direct_content and current_step["step_type"] == "recall"):
                    execution_result["result"] = f"æ— éœ€å·¥å…·è°ƒç”¨: {decision['reasoning']}"
                logger.info("No tool execution needed")
            
            # Update state - different logic for deep thinking mode
            updated_results = state.get("execution_results", []) + [execution_result]
            
            # Check if deep thinking mode is enabled
            deep_thinking = state.get("deep_thinking", False)
            
            if deep_thinking and current_step["step_type"] == "recall":
                # Deep thinking mode: generate QA pair for recall steps
                logger.info("Deep thinking mode: Generating QA pair for this step")
                
                try:
                    # Get previous QA pairs for context
                    previous_qa_pairs = state.get("qa_pairs", [])
                    previous_context = get_sub_question_context(previous_qa_pairs)
                    
                    # Prepare sub-question answer prompt
                    sub_question = current_step["title"]
                    recalled_content = execution_result["result"] if execution_result["result"] else "ï¼ˆæœªå¬å›åˆ°ç›¸å…³å†…å®¹ï¼‰"
                    
                    prompt = SUB_QUESTION_ANSWER_PROMPT.format(
                        user_query=user_query_with_context,
                        step_index=current_step_index + 1,
                        total_steps=len(plan["steps"]),
                        sub_question=sub_question,
                        previous_qa_context=previous_context,
                        recalled_content=recalled_content
                    )
                    
                    # Generate answer for this sub-question
                    response = self.llm.invoke([HumanMessage(content=prompt)])
                    sub_answer = response.content
                    
                    logger.info(f"Generated sub-answer, length: {len(sub_answer)} characters")
                    
                    # Create QA pair
                    qa_pair: QAPair = {
                        "step_index": current_step_index,
                        "question": sub_question,
                        "answer": sub_answer,
                        "recall_query": decision.get("query")
                    }
                    
                    updated_qa_pairs = previous_qa_pairs + [qa_pair]
                    
                    return {
                        "execution_results": updated_results,
                        "qa_pairs": updated_qa_pairs,
                        "current_step_index": current_step_index + 1
                    }
                    
                except Exception as e:
                    logger.error(f"Error generating QA pair: {str(e)}", exc_info=True)
                    # Fallback: treat as fast mode
                    logger.warning("Falling back to fast mode due to QA generation error")
                    deep_thinking = False
            
            # Fast mode or non-recall steps: accumulate information
            if not deep_thinking:
                updated_info = state.get("collected_information", "")
                if execution_result["result"] and not execution_result.get("error"):
                    updated_info += f"\n\nã€æ­¥éª¤ {current_step_index + 1}: {current_step['title']}ã€‘\n{execution_result['result']}"
                
                return {
                    "execution_results": updated_results,
                    "collected_information": updated_info,
                    "current_step_index": current_step_index + 1
                }
            else:
                # Deep thinking mode but non-recall step (analysis/synthesis)
                # Just move to next step without generating QA
                return {
                    "execution_results": updated_results,
                    "current_step_index": current_step_index + 1
                }
            
        except Exception as e:
            logger.error(f"Error in execution node: {str(e)}", exc_info=True)
            # Record error but continue
            error_result: ExecutionResult = {
                "step_index": current_step_index,
                "step_title": current_step["title"],
                "step_type": StepType(current_step["step_type"]),
                "tool_used": None,
                "query": None,
                "result": "",
                "error": str(e)
            }
            return {
                "execution_results": state.get("execution_results", []) + [error_result],
                "current_step_index": current_step_index + 1,
                "error": str(e)
            }
    
    def analysis_node(self, state: AgentState) -> Dict[str, Any]:
        """
        Analyze if collected information is sufficient.
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with analysis results
        """
        logger.info("============ Analysis Node ============")
        
        try:
            # Build execution summary
            execution_summary = "\n".join([
                f"æ­¥éª¤{r['step_index']+1}: {r['step_title']} - å·¥å…·: {r.get('tool_used', 'æ— ')}"
                for r in state.get("execution_results", [])
            ])
            
            # Prepare analysis prompt
            prompt = INFORMATION_ANALYSIS_PROMPT.format(
                user_query=state["user_query"],
                task_type=state["detected_intent"].value,
                collected_information=state.get("collected_information", ""),
                execution_summary=execution_summary
            )
            
            # Get analysis
            response = self.llm.invoke([HumanMessage(content=prompt)])
            analysis = parse_json_response(
                response.content,
                expected_fields=["is_sufficient", "analysis"]
            )
            
            if not analysis:
                logger.warning("Failed to parse analysis JSON, defaulting to sufficient")
                analysis = {
                    "is_sufficient": True,
                    "analysis": "æ— æ³•è§£æåˆ†æç»“æœï¼Œç»§ç»­ç”Ÿæˆç­”æ¡ˆ",
                    "missing_aspects": [],
                    "suggested_actions": []
                }
            
            is_sufficient = analysis["is_sufficient"]
            logger.info(f"Information sufficient: {is_sufficient}")
            logger.info(f"Analysis: {analysis['analysis'][:200]}...")
            
            # Check replan count
            replan_count = state.get("replan_count", 0)
            if not is_sufficient:
                if replan_count >= settings.max_replan_attempts:
                    logger.warning(f"Max replan attempts ({settings.max_replan_attempts}) reached, proceeding with available information")
                    is_sufficient = True
                    analysis["analysis"] += "\nï¼ˆå·²è¾¾æœ€å¤§é‡æ–°è§„åˆ’æ¬¡æ•°ï¼Œå°†åŸºäºç°æœ‰ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆï¼‰"
                else:
                    # Increment replan counter
                    replan_count += 1
                    logger.info(f"Information insufficient, replanning (attempt {replan_count}/{settings.max_replan_attempts})")
            
            return {
                "is_information_sufficient": is_sufficient,
                "analysis_result": analysis,
                "replan_count": replan_count
            }
            
        except Exception as e:
            logger.error(f"Error in analysis node: {str(e)}", exc_info=True)
            # Default to sufficient to avoid infinite loops
            return {
                "is_information_sufficient": True,
                "analysis_result": {
                    "is_sufficient": True,
                    "analysis": f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}ï¼Œå°†åŸºäºç°æœ‰ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ",
                    "missing_aspects": [],
                    "suggested_actions": []
                },
                "error": str(e)
            }
    
    def answer_generation_node(self, state: AgentState) -> Dict[str, Any]:
        """
        Generate final answer based on collected information or QA pairs.
        
        Args:
            state: Current agent state
            
        Returns:
            Updated state with final answer
        """
        logger.info("============ Answer Generation Node ============")
        logger.info(f"Intent: {state['detected_intent']}")
        
        deep_thinking = state.get("deep_thinking", False)
        logger.info(f"Mode: {'Deep Thinking' if deep_thinking else 'Fast'}")
        
        try:
            # Build context-aware query for answer generation
            user_query_with_context = state["user_query"]
            
            # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆ3è½® - answer generationéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
            context_str = self._get_conversation_context(state, num_turns=3, stage="answer_generation")
            
            if context_str:
                user_query_with_context = f"ã€å¯¹è¯å†å²ï¼ˆç”¨äºç†è§£ä¸Šä¸‹æ–‡å’Œä»£è¯ï¼‰ã€‘\n{context_str}\n\nã€å½“å‰é—®é¢˜ã€‘\n{state['user_query']}"
                logger.info("Using conversation history for answer generation (3 turns)")
            
            # Prepare context based on mode
            if deep_thinking:
                # Deep thinking mode: use QA pairs
                qa_pairs = state.get("qa_pairs", [])
                logger.info(f"Using {len(qa_pairs)} QA pairs for answer generation")
                
                # Format QA pairs as context
                if qa_pairs:
                    qa_context_lines = []
                    for i, qa in enumerate(qa_pairs, 1):
                        qa_context_lines.append(f"## å­é—®é¢˜ {i}: {qa['question']}\n")
                        qa_context_lines.append(f"{qa['answer']}\n")
                    context_for_llm = "\n".join(qa_context_lines)
                else:
                    context_for_llm = "ï¼ˆæ²¡æœ‰ç”ŸæˆQAå¯¹ï¼Œå¯èƒ½æ‰€æœ‰æ­¥éª¤éƒ½æ˜¯analysis/synthesisç±»å‹ï¼‰"
                    logger.warning("No QA pairs found in deep thinking mode")
            else:
                # Fast mode: use collected information
                context_for_llm = state.get("collected_information", "")
                logger.info(f"Using collected information, length: {len(context_for_llm)} chars")
            
            # Get the appropriate answer prompt
            prompt_template = get_answer_prompt(state["detected_intent"])
            prompt = prompt_template.format(
                user_query=user_query_with_context,
                collected_information=context_for_llm
            )
            
            # Generate answer
            response = self.llm.invoke([HumanMessage(content=prompt)])
            final_answer = response.content
            
            logger.info(f"Generated answer length: {len(final_answer)} characters")
            
            # Log QA pairs summary if in deep thinking mode
            if deep_thinking and qa_pairs:
                logger.info("=" * 60)
                logger.info("ğŸ“Š æ·±åº¦æ€è€ƒæ¨¡å¼ - QAå¯¹æ€»ç»“")
                logger.info("=" * 60)
                for i, qa in enumerate(qa_pairs, 1):
                    logger.info(f"Q{i}: {qa['question']}")
                    answer_preview = qa['answer'][:150] + "..." if len(qa['answer']) > 150 else qa['answer']
                    logger.info(f"A{i}: {answer_preview}")
                    logger.info("-" * 60)
            
            # ========================================================================
            # ä¼šè¯ç®¡ç†ï¼šä¿å­˜æ¶ˆæ¯å’Œæ£€æŸ¥å‹ç¼©
            # ========================================================================
            session_id = state.get('session_id')
            if session_id:
                # ğŸ”‘ å…³é”®ï¼šä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆåœ¨workflowç»“æŸæ—¶ä¿å­˜ï¼Œè€Œä¸æ˜¯å¼€å§‹æ—¶ï¼‰
                # è¿™æ ·ç¡®ä¿context injectionæ—¶ä¸ä¼šåŒ…å«å½“å‰çš„useræ¶ˆæ¯ï¼Œé¿å…é‡å¤
                if not state.get('_user_message_saved'):
                    self.session_manager.add_user_message(
                        session_id=session_id,
                        content=state["user_query"]
                    )
                    logger.info(f"User message saved to session {session_id}")
                
                # ä¿å­˜åŠ©æ‰‹å›å¤
                self.session_manager.add_assistant_message(
                    session_id=session_id,
                    content=final_answer
                )
                logger.debug(f"Assistant message saved to session {session_id}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
                if self.session_manager.should_compress(session_id):
                    logger.info(f"Triggering compression for session {session_id}")
                    compression_record = self.session_manager.trigger_compression(session_id)
                    logger.info(
                        f"Compression completed: saved {compression_record.saved_tokens} tokens, "
                        f"round {compression_record.round}"
                    )
            
            return {
                "final_answer": final_answer,
                "messages": state.get("messages", []) + [
                    AIMessage(content=final_answer)
                ]
            }
            
        except Exception as e:
            logger.error(f"Error in answer generation: {str(e)}", exc_info=True)
            raise RuntimeError(f"Answer generation failed: {str(e)}")

